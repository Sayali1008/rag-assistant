# RAG System Prototype

This repository contains a Retrieval-Augmented Generation (RAG) prototype built using LangChain, ChromaDB, and OpenAI's embedding and LLM APIs. It downloads the AWS Bedrock User Guide, embeds it into a vector store, and answers user queries using relevant document context.

---

## RAG Architecture Diagram
```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        User Query       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Vector Store    â”‚
              â”‚    (ChromaDB)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Similarity Search
                        â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Retrieved Relevant Chunks â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Context Assembly
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Prompt Template       â”‚
            â”‚ (Context + User Question) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  OpenAI LLM (gpt-4o-mini) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Generated Answer
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚       Final Output       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure
```
.
â”œâ”€â”€ main.py
â”œâ”€â”€ create_database.py
â”œâ”€â”€ query_data.py
â”œâ”€â”€ prompt.py
â”œâ”€â”€ data/              # Downloaded PDFs
â”œâ”€â”€ chroma/            # Persisted Chroma vector store
â””â”€â”€ requirements.txt
```

---

## Working

This repository implements a traditional Retrieveâ†’Augmentâ†’Generate pipeline.

### **1. Document Loading**

* The AWS Bedrock User Guide PDF is downloaded if not already present.
* The PDF is parsed page-by-page.

### **2. Text Chunking**

The PDF is split into overlapping text chunks:

* `chunk_size = 700`
* `chunk_overlap = 100`

Chunking allows the embedding model to process text while maintaining context.

### **3. Embedding and Vector Store Creation**

* Embeddings are generated with **OpenAI text-embedding-3-small**.
* ChromaDB is used as the persistent vector store.
* Chunks are embedded **in batches** to avoid API input-size errors.

ğŸ“Œ **Batch Size Notes:**

* The OpenAI embedding API limits input size per request.
* Sending too many chunks at once causes:
  `ValueError: Batch size ... exceeds max batch size ...`
* Therefore, chunk text must be small *and* embedding requests must be batched.

### **4. Retrieval**

For a given query:

* The vector store performs **semantic similarity search**.
* Returned items include `(Document, score)` pairs.

ğŸ“Œ **Score Notes:**

* The score is a *distance* metric.
* A **lower score** indicates a **closer match** to the query.

### **5. Context Packaging**

* Retrieved document chunks are formatted with metadata.
* Context is truncated if it exceeds `max_chars = 3000` to keep prompts manageable.

### **6. LLM Response Generation**

* A prompt template inserts the context and the user query.
* OpenAI `gpt-4o-mini` is used to generate a response.

